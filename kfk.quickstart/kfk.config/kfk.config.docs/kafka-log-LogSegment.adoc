== [[LogSegment]] LogSegment -- Record Segment Of Partition Log

`LogSegment` is a segment of <<log, records>> of a <<kafka-log-Log.adoc#, log>> of a <<kafka-cluster-Partition.adoc#, partition>>.

TIP: Use <<kafka-tools-DumpLogSegments.adoc#, DumpLogSegments>> tool to review the content of (the underlying files of) a log segment.

`LogSegment` is composed of two main file types, e.g. the <<log, log>> file itself (with records) and index files.

* <<lazyOffsetIndex, index>>

* <<lazyTimeIndex, time index>>

* <<txnIndex, transaction index>> (of <<updateTxnIndex, aborted transactions>>)

NOTE: When <<kafka-log-Log.adoc#, Log>> is created (and requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>>) orphaned index files are removed.

The files are all in the same directory (as specified when <<open, opening a segment>>).

```
$ tree /tmp/kafka-logs/t1-1/
/tmp/kafka-logs/t1-1/
├── 00000000000000000000.index
├── 00000000000000000000.log
├── 00000000000000000000.timeindex
└── leader-epoch-checkpoint
```

`LogSegment` is <<creating-instance, created>> (indirectly using <<open, LogSegment.open>> utility) when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>

.Example
[source, scala]
----
// One of log.dirs directories
import java.nio.file.Path
val dir = Path.of("/tmp/logsegment")
import java.nio.file.Files
Files.createDirectories(dir)
import kafka.log.{LogConfig, LogSegment}
import org.apache.kafka.common.utils.Time
val config = LogConfig()
val segment = LogSegment.open(dir.toFile, baseOffset = 0, config, time = Time.SYSTEM)
scala> println(segment)
LogSegment(baseOffset=0, size=0, lastModifiedTime=1576677605692, largestTime=0)
----

`LogSegment` uses the following configuration properties (while <<open, opening a log segment>>):

* <<kafka-log-LogConfig.adoc#maxIndexSize, segment.index.bytes>>

* <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>>

* <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.jitter.ms or segment.ms>>

=== [[creating-instance]] Creating LogSegment Instance

`LogSegment` takes the following to be created:

* [[log]] <<kafka-common-record-FileRecords.adoc#, FileRecords>>
* [[lazyOffsetIndex]] Lazy <<kafka-log-OffsetIndex.adoc#, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`)
* [[lazyTimeIndex]] Lazy <<kafka-log-TimeIndex.adoc#, TimeIndex>> with deferred loading (`LazyIndex[TimeIndex]`)
* <<txnIndex, TransactionIndex>>
* [[baseOffset]] Base offset
* [[indexIntervalBytes]] <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>>
* [[rollJitterMs]] <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.jitter.ms or segment.ms>>
* [[time]] `Time`

`LogSegment` initializes the <<internal-properties, internal properties>>.

=== [[open]] Opening Log Segment -- `open` Utility

[source, scala]
----
open(
  dir: File,
  baseOffset: Long,
  config: LogConfig,
  time: Time,
  fileAlreadyExists: Boolean = false,
  initFileSize: Int = 0,
  preallocate: Boolean = false,
  fileSuffix: String = ""): LogSegment
----

`open` uses the following configuration properties (of the given <<kafka-log-LogConfig.adoc#, LogConfig>>):

* <<kafka-log-LogConfig.adoc#maxIndexSize, segment.index.bytes>> for the `maxIndexSize`

* <<kafka-log-LogConfig.adoc#indexInterval, index.interval.bytes>> for the <<indexIntervalBytes, indexIntervalBytes>>

* <<kafka-log-LogConfig.adoc#randomSegmentJitter, segment.jitter.ms or segment.ms>> for the <<rollJitterMs, rollJitterMs>>

`open` creates a new <<creating-instance, LogSegment>> with the following files in the given `dir` directory:

* <<kafka-log-Log.adoc#logFile, Creates a log file>> (with the given `baseOffset`, and `fileSuffix`) and opens it (using `FileRecords.open`)

* <<kafka-log-Log.adoc#offsetIndexFile, Creates an offset index file>>

* <<kafka-log-Log.adoc#timeIndexFile, Creates a time index file>>

* <<kafka-log-Log.adoc#transactionIndexFile, Creates a transaction index file>>

[NOTE]
====
`open` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#loadSegmentFiles, loadSegmentFiles>> (for every log file), <<kafka-log-Log.adoc#completeSwapOperations, completeSwapOperations>>, <<kafka-log-Log.adoc#loadSegments, loadSegments>>, <<kafka-log-Log.adoc#recoverLog, recoverLog>>, <<kafka-log-Log.adoc#roll, roll>>, and <<kafka-log-Log.adoc#truncateFullyAndStartAt, truncateFullyAndStartAt>>

* `LogCleaner` is requested to <<kafka-log-LogCleaner.adoc#createNewCleanedSegment, createNewCleanedSegment>>
====

=== [[txnIndex]] LogSegment and TransactionIndex

[source, scala]
----
txnIndex: TransactionIndex
----

When <<creating-instance, created>>, `LogSegment` is given a <<kafka-log-TransactionIndex.adoc#, TransactionIndex>>.

`LogSegment` uses the `TransactionIndex` for the following:

* <<truncateTo, truncateTo>>

* <<updateTxnIndex, updateTxnIndex>>

* <<collectAbortedTxns, collectAbortedTxns>>

* <<flush, flush>>

* <<sanityCheck, sanityCheck>> and <<recover, recover>>

* <<updateDir, updateDir>> and <<changeFileSuffixes, changeFileSuffixes>>

`txnIndex` is also used when `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and in turn <<kafka-log-Cleaner.adoc#cleanSegments, cleanSegments>>)

`TransactionIndex` is <<kafka-log-TransactionIndex.adoc#close, closed>> when `LogSegment` is requested to <<close, close>> and <<closeHandlers, closeHandlers>>.

`TransactionIndex` is <<kafka-log-TransactionIndex.adoc#deleteIfExists, deleted (if exists)>> when `LogSegment` is requested to <<deleteIfExists, deleteIfExists>>.

=== [[offsetIndex]] LogSegment and OffsetIndex

[source, scala]
----
offsetIndex: OffsetIndex
----

When <<creating-instance, created>>, `LogSegment` is given an <<lazyOffsetIndex, OffsetIndex>> with deferred loading (`LazyIndex[OffsetIndex]`).

`offsetIndex` simply gets (_unwraps_) the <<kafka-log-OffsetIndex.adoc#, OffsetIndex>>.

`offsetIndex` is used for the following...FIXME

=== [[collectAbortedTxns]] `collectAbortedTxns` Method

[source, scala]
----
collectAbortedTxns(
  fetchOffset: Long,
  upperBoundOffset: Long): TxnIndexSearchResult
----

`collectAbortedTxns`...FIXME

NOTE: `collectAbortedTxns` is used when `Log` is requested to <<kafka-log-Log.adoc#collectAbortedTransactions, collectAbortedTransactions>>.

=== [[close]] Closing -- `close` Method

[source, scala]
----
close(): Unit
----

`close`...FIXME

NOTE: `close` is used when `Log` is requested to <<kafka-log-Log.adoc#loadSegments, load segments>>, <<kafka-log-Log.adoc#close, close>>, and <<kafka-log-Log.adoc#splitOverflowedSegment splitOverflowedSegment>>.

=== [[closeHandlers]] `closeHandlers` Method

[source, scala]
----
closeHandlers(): Unit
----

`closeHandlers`...FIXME

NOTE: `closeHandlers` is used when `Log` is requested to <<kafka-log-Log.adoc#closeHandlers, closeHandlers>>.

=== [[recover]] Recovering -- `recover` Method

[source, scala]
----
recover(
  producerStateManager: ProducerStateManager,
  leaderEpochCache: Option[LeaderEpochFileCache] = None): Int
----

`recover` requests the <<offsetIndex, OffsetIndex>>, the <<timeIndex, TimeIndex>>, and the <<txnIndex, TransactionIndex>> to <<kafka-log-AbstractIndex.adoc#reset, reset>>.

NOTE: `recover` is used when `Log` is requested to <<kafka-log-Log.adoc#recoverSegment, recover a log segment>>.

==== [[updateProducerState]] `updateProducerState` Internal Method

[source, scala]
----
updateProducerState(
  producerStateManager: ProducerStateManager,
  batch: RecordBatch): Unit
----

`updateProducerState`...FIXME

NOTE: `updateProducerState` is used when `LogSegment` is requested to <<recover, recover>>.

=== [[sanityCheck]] `sanityCheck` Method

[source, scala]
----
sanityCheck(
  timeIndexFileNewlyCreated: Boolean): Unit
----

`sanityCheck`...FIXME

NOTE: `sanityCheck` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#loadSegments, loadSegments>> (when <<kafka-log-Log.adoc#creating-instance-loadSegments, created>>).

=== [[updateDir]] `updateDir` Method

[source, scala]
----
updateDir(
  dir: File): Unit
----

`updateDir`...FIXME

NOTE: `updateDir` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#renameDir, renameDir>>.

=== [[changeFileSuffixes]] `changeFileSuffixes` Method

[source, scala]
----
changeFileSuffixes(
  oldSuffix: String,
  newSuffix: String): Unit
----

`changeFileSuffixes`...FIXME

NOTE: `changeFileSuffixes` is used when `Log` is requested to <<kafka-log-Log.adoc#asyncDeleteSegment, asyncDeleteSegment>> and <<kafka-log-Log.adoc#replaceSegments, replaceSegments>>.

=== [[flush]] `flush` Method

[source, scala]
----
flush(): Unit
----

`flush`...FIXME

[NOTE]
====
`flush` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#flush, flush>> and <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanSegments, cleanSegments>>)
====

=== [[deleteIfExists]] `deleteIfExists` Method

[source, scala]
----
deleteIfExists(): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[deleteIfExists-utility]] `deleteIfExists` Utility

[source, scala]
----
deleteIfExists(
  dir: File,
  baseOffset: Long,
  fileSuffix: String = ""): Unit
----

`deleteIfExists`...FIXME

NOTE: `deleteIfExists` is used when...FIXME

=== [[resizeIndexes]] `resizeIndexes` Method

[source, scala]
----
resizeIndexes(
  size: Int): Unit
----

`resizeIndexes`...FIXME

NOTE: `resizeIndexes` is used when...FIXME

=== [[largestTimestamp]] `largestTimestamp` Method

[source, scala]
----
largestTimestamp: Long
----

`largestTimestamp`...FIXME

NOTE: `largestTimestamp` is used when...FIXME

=== [[shouldRoll]] `shouldRoll` Method

[source, scala]
----
shouldRoll(
  rollParams: RollParams): Boolean
----

`shouldRoll`...FIXME

NOTE: `shouldRoll` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#maybeRoll, maybeRoll>> (while <<kafka-log-Log.adoc#append, appending records>>).

=== [[timeWaitedForRoll]] `timeWaitedForRoll` Method

[source, scala]
----
timeWaitedForRoll(
  now: Long,
  messageTimestamp: Long) : Long
----

`timeWaitedForRoll`...FIXME

NOTE: `timeWaitedForRoll` is used exclusively when `LogSegment` is requested to <<shouldRoll, shouldRoll>>.

=== [[append]] `append` Method

[source, scala]
----
append(
  largestOffset: Long,
  largestTimestamp: Long,
  shallowOffsetOfMaxTimestamp: Long,
  records: MemoryRecords): Unit
----

`append`...FIXME

[NOTE]
====
`append` is used exclusively when:

* `Log` is requested to <<kafka-log-Log.adoc#append, append records>>

* `Cleaner` is requested to <<kafka-log-Cleaner.adoc#clean, clean a log>> (and <<kafka-log-Cleaner.adoc#cleanInto, cleanInto>>)

* `LogSegment` is requested to <<appendChunkFromFile, appendChunkFromFile>>
====

=== [[appendFromFile]] `appendFromFile` Method

[source, scala]
----
appendFromFile(
  records: FileRecords,
  start: Int): Int
----

`appendFromFile`...FIXME

NOTE: `appendFromFile` is used exclusively when `Log` is requested to <<kafka-log-Log.adoc#splitOverflowedSegment, splitOverflowedSegment>>.

==== [[appendChunkFromFile]] `appendChunkFromFile` Internal Method

[source, scala]
----
appendChunkFromFile(
  records: FileRecords,
  position: Int,
  bufferSupplier: BufferSupplier): Int
----

`appendChunkFromFile`...FIXME

NOTE: `appendChunkFromFile` is used when `LogSegment` is requested to <<appendFromFile, appendFromFile>>.

=== [[truncateTo]] Truncating To Offset -- `truncateTo` Method

[source, scala]
----
truncateTo(
  offset: Long): Int
----

`truncateTo`...FIXME

NOTE: `truncateTo` is used when `Log` is <<kafka-log-Log.adoc#creating-instance-loadSegments, created>> (and in turn <<kafka-log-Log.adoc#recoverLog, recoverLog>>) and <<kafka-log-Log.adoc#truncateTo, truncateTo>>.

=== [[updateTxnIndex]] `updateTxnIndex` Method

[source, scala]
----
updateTxnIndex(
  completedTxn: CompletedTxn,
  lastStableOffset: Long): Unit
----

`updateTxnIndex`...FIXME

[NOTE]
====
`updateTxnIndex` is used when:

* `Log` is requested to <<kafka-log-Log.adoc#append, append records>>

* `LogSegment` is requested to <<updateProducerState, updateProducerState>>
====

=== [[size]] `size` Method

[source, scala]
----
size: Int
----

`size` requests the <<log, FileRecords>> for the link:kafka-common-record-FileRecords.adoc#sizeInBytes[size (in bytes)].

[NOTE]
====
`size` is used when...FIXME
====

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| created
a| [[created]] Time(stamp) when this `LogSegment` was <<creating-instance, created>> or <<truncateTo, truncated completely>> (to `0`)

Used exclusively when `LogSegment` is requested for the <<timeWaitedForRoll, time it has waited to be rolled>>

| bytesSinceLastIndexEntry
a| [[bytesSinceLastIndexEntry]]

Used when...FIXME

| rollingBasedTimestamp
a| [[rollingBasedTimestamp]]

Used when...FIXME

| _maxTimestampSoFar
a| [[_maxTimestampSoFar]]

Used when...FIXME

| _offsetOfMaxTimestampSoFar
a| [[_offsetOfMaxTimestampSoFar]]

Used when...FIXME

|===
