== [[RecordAccumulator]] RecordAccumulator

`RecordAccumulator` is <<creating-instance, created>> exclusively when `KafkaProducer` is <<kafka-producer-KafkaProducer.adoc#accumulator, created>>.

`KafkaProducer` uses the following configuration properties to create a `RecordAccumulator`:

* <<kafka-producer-ProducerConfig.adoc#batch.size, batch.size>> for the <<batchSize, batchSize>>

* <<kafka-producer-ProducerConfig.adoc#linger.ms, linger.ms>> for the <<lingerMs, lingerMs>>

* <<kafka-producer-ProducerConfig.adoc#retry.backoff.ms, retry.backoff.ms>> for the <<retryBackoffMs, retryBackoffMs>>

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.kafka.clients.producer.internals.RecordAccumulator` logger to see what happens inside.

Add the following line to `config/tools-log4j.properties`:

```
log4j.logger.org.apache.kafka.clients.producer.internals.RecordAccumulator=ALL
```

Refer to <<kafka-logging.adoc#, Logging>>.
====

=== [[creating-instance]] Creating RecordAccumulator Instance

`RecordAccumulator` takes the following to be created:

* [[logContext]] `LogContext`
* [[batchSize]] `batchSize`
* [[compression]] `CompressionType`
* [[lingerMs]] `lingerMs`
* [[retryBackoffMs]] `retryBackoffMs`
* [[deliveryTimeoutMs]] `deliveryTimeoutMs`
* [[metrics]] <<kafka-Metrics.adoc#, Metrics>>
* [[metricGrpName]] Metric group name (e.g. <<kafka-producer-KafkaProducer.adoc#PRODUCER_METRIC_GROUP_NAME, producer-metrics>>)
* [[time]] `Time`
* [[apiVersions]] `ApiVersions`
* [[transactionManager]] <<kafka-producer-internals-TransactionManager.adoc#, TransactionManager>>
* [[bufferPool]] `BufferPool`

`RecordAccumulator` initializes the <<internal-properties, internal properties>>.

When created, `RecordAccumulator` <<registerMetrics, registerMetrics>> (with the <<metrics, Metrics>> and the <<metricGrpName, metric group name>>).

==== [[registerMetrics]] `registerMetrics` Method

[source, java]
----
void registerMetrics()
----

`registerMetrics`...FIXME

NOTE: `registerMetrics` is used exclusively when `RecordAccumulator` is <<creating-instance, created>>.

=== [[close]] `close` Method

[source, java]
----
void close()
----

`close`...FIXME

NOTE: `close` is used when...FIXME

=== [[append]] `append` Method

[source, java]
----
RecordAppendResult append(
  TopicPartition tp,
  long timestamp,
  byte[] key,
  byte[] value,
  Header[] headers,
  Callback callback,
  long maxTimeToBlock) throws InterruptedException
----

`append` <<getOrCreateDeque, looks up or creates a new Deque of ProducerBatches for a given TopicPartition>>.

`append` <<tryAppend, tryAppend>> and returns the `RecordAppendResult` if available (not `null`).

`append` prints out the following TRACE message to the logs:

```
Allocating a new [size] byte message buffer for topic [topic] partition [partition]
```

NOTE: The `size` in the above TRACE message is controlled by <<kafka-producer-ProducerConfig.adoc#batch.size, batch.size>> producer configuration property (default: `16384` which is `16 * 1024`).

`append` once again <<tryAppend, tryAppend>> and returns the `RecordAppendResult` if available (which they say _"should not happen often"_).

`append` <<recordsBuilder, creates a new MemoryRecordsBuilder>> and a new <<kafka-producer-internals-ProducerBatch.adoc#, ProducerBatch>> (for the given `TopicPartition` and the new `MemoryRecordsBuilder`).

`append` requests the `ProducerBatch` to <<kafka-producer-internals-ProducerBatch.adoc#tryAppend, tryAppend>> (that gives a `FutureRecordMetadata`).

`append` adds the `ProducerBatch` to the `Deque<ProducerBatch>` and the <<incomplete, IncompleteBatches>> internal registry.

In the end, `append` creates a new `RecordAppendResult` (with the `FutureRecordMetadata`).

NOTE: `append` is used exclusively when `KafkaProducer` is requested to <<kafka-producer-KafkaProducer.adoc#doSend, send a ProducerRecord to a Kafka Cluster asynchronously>>.

==== [[tryAppend]] `tryAppend` Internal Method

[source, java]
----
RecordAppendResult tryAppend(
  long timestamp,
  byte[] key,
  byte[] value,
  Header[] headers,
  Callback callback,
  Deque<ProducerBatch> deque)
----

`tryAppend`...FIXME

NOTE: `tryAppend` is used exclusively when `RecordAccumulator` is requested to <<append, append>>.

==== [[recordsBuilder]] Creating MemoryRecordsBuilder Instance (With TimestampType.CREATE_TIME) -- `recordsBuilder` Internal Method

[source, java]
----
MemoryRecordsBuilder recordsBuilder(
  ByteBuffer buffer,
  byte maxUsableMagic)
----

`recordsBuilder` simply builds a new <<kafka-common-record-MemoryRecordsBuilder.adoc#, MemoryRecordsBuilder>> (with `TimestampType.CREATE_TIME`).

`recordsBuilder` throws an `UnsupportedVersionException` when the <<transactionManager, TransactionManager>> is defined and the `maxUsableMagic` magic number is lower than `2`:

```
Attempting to use idempotence with a broker which does not support the required message format (v2). The broker must be version 0.11 or later.
```

NOTE: `recordsBuilder` is used exclusively when `RecordAccumulator` is requested to <<append, append>>.

=== [[abortIncompleteBatches]] Aborting Incomplete Batches -- `abortIncompleteBatches` Method

[source, java]
----
void abortIncompleteBatches()
----

`abortIncompleteBatches`...FIXME

NOTE: `abortIncompleteBatches` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#run, run>> (and forced to close while shutting down).

=== [[reenqueue]] `reenqueue` Method

[source, java]
----
void reenqueue(
  ProducerBatch batch,
  long now)
----

`reenqueue`...FIXME

NOTE: `reenqueue` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#reenqueueBatch, reenqueueBatch>>.

=== [[splitAndReenqueue]] `splitAndReenqueue` Method

[source, java]
----
int splitAndReenqueue(ProducerBatch bigBatch)
----

`splitAndReenqueue`...FIXME

NOTE: `splitAndReenqueue` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#completeBatch, completeBatch>>.

=== [[getOrCreateDeque]] Looking Up Or Creating New Deque Of ProducerBatches For TopicPartition -- `getOrCreateDeque` Internal Method

[source, java]
----
Deque<ProducerBatch> getOrCreateDeque(TopicPartition tp)
----

`getOrCreateDeque`...FIXME

NOTE: `getOrCreateDeque` is used when `RecordAccumulator` is requested to <<append, append>>, <<reenqueue, reenqueue>>, and <<splitAndReenqueue, splitAndReenqueue>>.

=== [[expiredBatches]] `expiredBatches` Method

[source, java]
----
List<ProducerBatch> expiredBatches(long now)
----

`expiredBatches`...FIXME

NOTE: `expiredBatches` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#sendProducerData, sendProducerData>>.

=== [[ready]] `ready` Method

[source, java]
----
ReadyCheckResult ready(
  Cluster cluster,
  long nowMs)
----

`ready`...FIXME

NOTE: `ready` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#sendProducerData, sendProducerData>> (when requested to <<kafka-producer-internals-Sender.adoc#runOnce, run a single iteration of sending>>).

=== [[drain]] `drain` Method

[source, java]
----
Map<Integer, List<ProducerBatch>> drain(
  Cluster cluster,
  Set<Node> nodes,
  int maxSize,
  long now)
----

`drain`...FIXME

NOTE: `drain` is used exclusively when `Sender` is requested to <<kafka-producer-internals-Sender.adoc#sendProducerData, sendProducerData>> (when requested to <<kafka-producer-internals-Sender.adoc#runOnce, run a single iteration of sending>>).

==== [[drainBatchesForOneNode]] `drainBatchesForOneNode` Internal Method

[source, java]
----
List<ProducerBatch> drainBatchesForOneNode(
  Cluster cluster,
  Node node,
  int maxSize,
  long now)
----

`drainBatchesForOneNode`...FIXME

NOTE: `drainBatchesForOneNode` is used exclusively when `RecordAccumulator` is requested to <<drain, drain>>.

=== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| appendsInProgress
a| [[appendsInProgress]] https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/atomic/AtomicInteger.html[java.util.concurrent.atomic.AtomicInteger] to keep track of the number of <<append, appending threads>>

* Starts at `0` when `RecordAccumulator` is <<creating-instance, created>>

* Increments and decrements when `RecordAccumulator` is requested to <<append, append>> (just after it is requested and right before it finishes)

Used exclusively when `RecordAccumulator` is requested to <<abortIncompleteBatches, abort incomplete batches>>

| batches
a| [[batches]] <<kafka-producer-internals-ProducerBatch.adoc#, ProducerBatches>> per `TopicPartition` (`ConcurrentMap<TopicPartition, Deque<ProducerBatch>>`)

| incomplete
a| [[incomplete]] `IncompleteBatches`

|===
